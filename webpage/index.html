<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>ABAP LLM Benchmark Leaderboard</title>
    <meta
      name="description"
      content="ABAP LLM Benchmark Leaderboard for SAP ABAP code generation. Compare models across 180 tasks x 10 repetitions with up to 5 feedback rounds (R0-R5) and pass@5."
    />
    <link rel="canonical" href="https://abap-llm-benchmark.marianzeis.de/" />
    <meta
      name="robots"
      content="index,follow,max-image-preview:large,max-snippet:-1,max-video-preview:-1"
    />
    <meta
      name="googlebot"
      content="index,follow,max-image-preview:large,max-snippet:-1,max-video-preview:-1"
    />
    <meta name="theme-color" content="#060912" />
    <link rel="icon" href="favicon.svg" type="image/svg+xml" />

    <meta property="og:site_name" content="ABAP LLM Benchmark" />
    <meta property="og:type" content="website" />
    <meta property="og:title" content="ABAP LLM Benchmark Leaderboard" />
    <meta
      property="og:description"
      content="ABAP LLM Benchmark Leaderboard for SAP ABAP code generation. Compare models across 180 tasks x 10 repetitions with up to 5 feedback rounds (R0-R5) and pass@5."
    />
    <meta property="og:url" content="https://abap-llm-benchmark.marianzeis.de/" />

    <meta name="twitter:card" content="summary" />
    <meta name="twitter:title" content="ABAP LLM Benchmark Leaderboard" />
    <meta
      name="twitter:description"
      content="ABAP LLM Benchmark Leaderboard for SAP ABAP code generation. Compare models across 180 tasks x 10 repetitions with up to 5 feedback rounds (R0-R5) and pass@5."
    />

    <link rel="preconnect" href="https://fonts.googleapis.com" />
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
    <link
      rel="stylesheet"
      href="https://fonts.googleapis.com/css2?family=IBM+Plex+Sans:wght@400;500;600;700&family=Space+Grotesk:wght@500;700&display=swap"
    />
    <link rel="stylesheet" href="styles.css" />

    <script type="application/ld+json">
      {
        "@context": "https://schema.org",
        "@type": "WebPage",
        "name": "ABAP LLM Benchmark Leaderboard",
        "url": "https://abap-llm-benchmark.marianzeis.de/",
        "description": "ABAP LLM Benchmark Leaderboard for SAP ABAP code generation. Compare models across 180 tasks x 10 repetitions with up to 5 feedback rounds (R0-R5) and pass@5.",
        "inLanguage": "en",
        "isPartOf": {
          "@type": "WebSite",
          "name": "ABAP LLM Benchmark",
          "url": "https://abap-llm-benchmark.marianzeis.de/"
        },
        "about": [
          {
            "@type": "Thing",
            "name": "SAP ABAP"
          },
          {
            "@type": "Thing",
            "name": "Large Language Models"
          }
        ]
      }
    </script>

    <script type="application/ld+json">
      {
        "@context": "https://schema.org",
        "@type": "Dataset",
        "name": "ABAP LLM Benchmark — Results and Leaderboard",
        "description": "Benchmark dataset for LLM-generated SAP ABAP code across 180 tasks x 10 repetitions with feedback rounds (R0-R5).",
        "url": "https://abap-llm-benchmark.marianzeis.de/",
        "license": "https://opensource.org/licenses/MIT",
        "creator": [
          {
            "@type": "Person",
            "name": "Tim Köhne"
          },
          {
            "@type": "Person",
            "name": "Marian Zeis"
          }
        ],
        "keywords": ["ABAP", "SAP", "LLM", "benchmark", "code generation"],
        "isBasedOn": "https://arxiv.org/abs/2601.15188",
        "sameAs": "https://github.com/marianfoo/LLM-Benchmark-ABAP-Code-Generation",
        "distribution": [
          {
            "@type": "DataDownload",
            "encodingFormat": "text/csv",
            "contentUrl": "https://abap-llm-benchmark.marianzeis.de/data/model_leaderboard.csv"
          },
          {
            "@type": "DataDownload",
            "encodingFormat": "text/csv",
            "contentUrl": "https://abap-llm-benchmark.marianzeis.de/data/results.csv"
          },
          {
            "@type": "DataDownload",
            "encodingFormat": "application/json",
            "contentUrl": "https://abap-llm-benchmark.marianzeis.de/data/dashboard.json"
          },
          {
            "@type": "DataDownload",
            "encodingFormat": "application/json",
            "contentUrl": "https://abap-llm-benchmark.marianzeis.de/data/syntax_errors.json"
          }
        ]
      }
    </script>
  </head>
  <body>
    <div class="page-bg"></div>
    <main class="layout">
      <header class="hero card">
        <p class="eyebrow">ABAP Code Generation Benchmark</p>
        <p class="abap-highlight">SAP ABAP LLM MODEL TESTING</p>
        <h1>ABAP LLM Model Leaderboard</h1>
        <p class="intro">
          This dashboard benchmarks LLMs specifically on SAP ABAP code generation
          across 180 tasks and 10 repetitions per task, with up to 5 feedback
          iterations.
          Baseline methodology comes from the original paper:
          <a href="https://arxiv.org/abs/2601.15188" target="_blank" rel="noopener noreferrer">
            Benchmarking Large Language Models for ABAP Code Generation (2601.15188)
          </a>.
        </p>
      </header>

      <noscript>
        <section class="card">
          <h2>JavaScript is required to render the tables</h2>
          <p class="intro">
            If you cannot enable JavaScript, you can still download the raw benchmark artifacts:
            <a href="data/model_leaderboard.csv">model leaderboard (CSV)</a>,
            <a href="data/results.csv">results (CSV)</a>, and
            <a href="data/syntax_errors.json">syntax errors (JSON)</a>.
          </p>
        </section>
      </noscript>

      <section class="card">
        <div class="section-title-row">
          <h2>Main Comparison Table</h2>
          <p class="section-subtitle">
            Only fully evaluated models are shown (Max rounds tested = 6). Sort by any column and filter by model name.
          </p>
        </div>
        <div class="controls">
          <label class="control">
            <span>Filter Model</span>
            <input id="modelFilter" type="search" placeholder="e.g. gpt, claude, qwen" />
          </label>
          <button id="resetFilters" class="btn" type="button">Reset</button>
        </div>
        <div class="table-wrap">
          <table id="mainTable">
            <caption class="sr-only">Sortable main comparison table of evaluated ABAP LLM models.</caption>
            <thead></thead>
            <tbody></tbody>
          </table>
        </div>
      </section>

      <section class="card">
        <h2>Methodology Notes</h2>
        <div class="notes-grid">
          <article>
            <h3>What Is Reused From The Paper</h3>
            <ul>
              <li>Success progression by feedback round (R0 to R5).</li>
              <li>Success rates by benchmark task category.</li>
              <li>Error-stage perspective based on SAP validation order.</li>
            </ul>
          </article>
          <article>
            <h3>What Is Extended Here</h3>
            <ul>
              <li>Sortable developer-facing leaderboard table.</li>
              <li>pass@5, AUC over rounds, and prompt consistency metrics.</li>
              <li>Completeness signal (`Max rounds tested`) for fair comparison.</li>
            </ul>
          </article>
          <article>
            <h3>Generated Artifacts</h3>
            <ul>
              <li><a href="data/model_leaderboard.csv">`data/model_leaderboard.csv`</a></li>
              <li><a href="data/results.csv">`data/results.csv`</a></li>
              <li><a href="data/syntax_errors.json">`data/syntax_errors.json`</a></li>
            </ul>
          </article>
        </div>
      </section>

      <section class="card">
        <h2>Cumulative Success By Feedback Round</h2>
        <div class="table-wrap">
          <table id="roundTable">
            <caption class="sr-only">Cumulative success rates by feedback round (R0 to R5).</caption>
            <thead></thead>
            <tbody></tbody>
          </table>
        </div>
      </section>

      <section class="card">
        <h2>Success By Task Category</h2>
        <div class="table-wrap">
          <table id="categoryTable">
            <caption class="sr-only">Final success rates by benchmark task category.</caption>
            <thead></thead>
            <tbody></tbody>
          </table>
        </div>
      </section>

      <section class="card">
        <h2>Plots</h2>
        <div id="plotsGrid" class="plots-grid"></div>
      </section>
    </main>

    <template id="plotCardTemplate">
      <article class="plot-card">
        <a class="plot-image-link" target="_blank" rel="noopener noreferrer">
          <img alt="" loading="lazy" />
        </a>
        <h3></h3>
        <p></p>
      </article>
    </template>

    <script src="app.js?v=20260209-1" defer></script>
  </body>
</html>
