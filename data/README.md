# Data Directory

This directory contains all benchmark outputs, intermediate artifacts, and
derived publish files. The **single source of truth** for each model's
benchmark run is `<model>.json`.

## File types

### Source-of-truth conversation logs

| Pattern | Description |
|---------|-------------|
| `<model>.json` | Full conversation logs (system prompt + user/assistant messages, including SAP compiler and unit-test feedback). One entry per prompt x repetition. **Do not delete** -- everything else is derived from these files. |

### Batch API transport artifacts

| Pattern | Description |
|---------|-------------|
| `<model>_batch.jsonl` | Batch API input file (requests sent to the provider). Recreated on each generation run. |
| `<model>_batch_response.jsonl` | Batch API response file (provider replies). Recreated on each generation run. |
| `anthropic_batch_tracking.json` | Tracks pending/completed Anthropic batch jobs. Safe to delete if no batches are in flight. |
| `openai_batch_tracking.json` | Tracks pending/completed OpenAI batch jobs. Safe to delete if no batches are in flight. |

### Derived runtime state

| Pattern | Description |
|---------|-------------|
| `<model>_tiers.json` | SAP/ADT tier results per feedback round. Regenerated by SAP testing. |
| `<model>_retry_state.json` | Retry attempt counters. Deleting resets the retry budget. |
| `<model>_queue.json` | Parallel runner work queue. Recreated on each test run. |
| `<model>_abap_test_failures.log` | Append-only failure log. Informational only. |

### Derived publish artifacts

| File | Description |
|------|-------------|
| `results.csv` | Consolidated benchmark results (one row per model x prompt x repetition). Generated by `analysis/consolidate_results.py`. |
| `syntax_errors.json` | Categorized syntax error counts per model. Generated by `analysis/generate_syntax_errors.py`. |
| `model_leaderboard.csv` | Leaderboard metrics for all models. Generated by `analysis/generate_leaderboard.py`. |
| `prompt_classification.csv` | Task category labels for each prompt (input, not generated). |
| `smoke_test_batch.jsonl` | Small test file for batch API smoke tests. |

## Regenerating derived files

All derived publish artifacts can be rebuilt from the conversation logs with a
single command (no SAP system required):

```bash
python analysis/build_publish_assets.py
```

This regenerates `results.csv`, `syntax_errors.json`, `model_leaderboard.csv`,
all plots in `plots/`, and website data in `webpage/`.

## Privacy and security

- **No API keys, tokens, or credentials should ever appear in this directory.**
  All secrets are loaded from `.env` (which is gitignored) at runtime.
- The batch tracking files contain provider-assigned batch IDs (e.g.
  `batch_...`, `msgbatch_...`). These are not secrets -- they cannot be used
  without a valid API key -- but they are operational metadata.
- Conversation logs contain only LLM-generated ABAP code and SAP compiler /
  unit-test error messages. No personal data is included.
